{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firefox driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated completion of scraping data for test_Ecoli-2: 2022-01-28 08:04:43.644229, in 4.75 hours\n",
      "The Phosphofructokinase is duplicated in the model\n",
      "The Pyruvate formate lyase is duplicated in the model\n",
      "The Glucose-6-phosphate isomerase is duplicated in the model\n",
      "The Phosphoglycerate kinase is duplicated in the model\n",
      "The 6-phosphogluconolactonase is duplicated in the model\n",
      "The Acetaldehyde dehydrogenase (acetylating) is duplicated in the model\n",
      "The 2 oxoglutarate reversible transport via symport is duplicated in the model\n",
      "The Phosphoglycerate mutase is duplicated in the model\n",
      "The Phosphate reversible transport via symport is duplicated in the model\n"
     ]
    }
   ],
   "source": [
    "# %run \"../../../../biofouling_models/Web scraping and data/SABIO/complete_sabio.py\"\n",
    "%run ../bigg_sabio/scrapper.py\n",
    "scraping = SABIO_scraping()\n",
    "scraping.main('../bigg_models/Ecoli core, BiGG, indented.json', 'test_Ecoli-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'In': ['',\n",
      "        'from pprint import pprint\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        'values = {\\n'\n",
      "        \"    'a': 120,\\n\"\n",
      "        \"    'b': 10\\n\"\n",
      "        '}\\n'\n",
      "        'locals().update(values)\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        \"print(eval('a*b'))\"],\n",
      " 'Out': {},\n",
      " '_': '',\n",
      " '__': '',\n",
      " '___': '',\n",
      " '__builtin__': <module 'builtins' (built-in)>,\n",
      " '__builtins__': <module 'builtins' (built-in)>,\n",
      " '__doc__': 'Automatically created module for IPython interactive environment',\n",
      " '__loader__': None,\n",
      " '__name__': '__main__',\n",
      " '__package__': None,\n",
      " '__spec__': None,\n",
      " '_dh': ['C:\\\\Users\\\\Andrew Freiburger\\\\Dropbox\\\\My PC '\n",
      "         '(DESKTOP-M302P50)\\\\Documents\\\\UVic Civil '\n",
      "         'Engineering\\\\dFBA\\\\BiGG_SABIO\\\\examples'],\n",
      " '_i': '',\n",
      " '_i1': 'from pprint import pprint\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        'values = {\\n'\n",
      "        \"    'a': 120,\\n\"\n",
      "        \"    'b': 10\\n\"\n",
      "        '}\\n'\n",
      "        'locals().update(values)\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        \"print(eval('a*b'))\",\n",
      " '_ih': ['',\n",
      "         'from pprint import pprint\\n'\n",
      "         'pprint(locals())\\n'\n",
      "         'values = {\\n'\n",
      "         \"    'a': 120,\\n\"\n",
      "         \"    'b': 10\\n\"\n",
      "         '}\\n'\n",
      "         'locals().update(values)\\n'\n",
      "         'pprint(locals())\\n'\n",
      "         \"print(eval('a*b'))\"],\n",
      " '_ii': '',\n",
      " '_iii': '',\n",
      " '_oh': {},\n",
      " 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x0000026DA7C7F988>,\n",
      " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000026DA7BCED08>>,\n",
      " 'pprint': <function pprint at 0x0000026DA5CE8B88>,\n",
      " 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x0000026DA7C7F988>}\n",
      "{'In': ['',\n",
      "        'from pprint import pprint\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        'values = {\\n'\n",
      "        \"    'a': 120,\\n\"\n",
      "        \"    'b': 10\\n\"\n",
      "        '}\\n'\n",
      "        'locals().update(values)\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        \"print(eval('a*b'))\"],\n",
      " 'Out': {},\n",
      " '_': '',\n",
      " '__': '',\n",
      " '___': '',\n",
      " '__builtin__': <module 'builtins' (built-in)>,\n",
      " '__builtins__': <module 'builtins' (built-in)>,\n",
      " '__doc__': 'Automatically created module for IPython interactive environment',\n",
      " '__loader__': None,\n",
      " '__name__': '__main__',\n",
      " '__package__': None,\n",
      " '__spec__': None,\n",
      " '_dh': ['C:\\\\Users\\\\Andrew Freiburger\\\\Dropbox\\\\My PC '\n",
      "         '(DESKTOP-M302P50)\\\\Documents\\\\UVic Civil '\n",
      "         'Engineering\\\\dFBA\\\\BiGG_SABIO\\\\examples'],\n",
      " '_i': '',\n",
      " '_i1': 'from pprint import pprint\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        'values = {\\n'\n",
      "        \"    'a': 120,\\n\"\n",
      "        \"    'b': 10\\n\"\n",
      "        '}\\n'\n",
      "        'locals().update(values)\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        \"print(eval('a*b'))\",\n",
      " '_ih': ['',\n",
      "         'from pprint import pprint\\n'\n",
      "         'pprint(locals())\\n'\n",
      "         'values = {\\n'\n",
      "         \"    'a': 120,\\n\"\n",
      "         \"    'b': 10\\n\"\n",
      "         '}\\n'\n",
      "         'locals().update(values)\\n'\n",
      "         'pprint(locals())\\n'\n",
      "         \"print(eval('a*b'))\"],\n",
      " '_ii': '',\n",
      " '_iii': '',\n",
      " '_oh': {},\n",
      " 'a': 120,\n",
      " 'b': 10,\n",
      " 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x0000026DA7C7F988>,\n",
      " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000026DA7BCED08>>,\n",
      " 'pprint': <function pprint at 0x0000026DA5CE8B88>,\n",
      " 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x0000026DA7C7F988>,\n",
      " 'values': {'a': 120, 'b': 10}}\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(locals())\n",
    "values = {\n",
    "    'a': 120,\n",
    "    'b': 10\n",
    "}\n",
    "locals().update(values)\n",
    "pprint(locals())\n",
    "print(eval('a*b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-f743a63a004c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-f743a63a004c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    for x: str in range(10):\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for x: str in range(10):\n",
    "    print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r",
      "1\r",
      "2\r",
      "3\r",
      "4\r",
      "5\r",
      "6\r",
      "7\r",
      "8\r",
      "9\r"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    print(x, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "products.append(6)\n",
    "products[-1] = 4\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  4  5\n",
       "1  7  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.DataFrame(data = [[4,5], [7,8]])\n",
    "\n",
    "display(df)\n",
    "\n",
    "print(df[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self,source='sabio'):\n",
    "\n",
    "        self.source = 'sabio'\n",
    "            \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "        \n",
    "    def wait_for_id(self,n_id):\n",
    "        while True:\n",
    "            try:\n",
    "                element = self.driver.find_element_by_id(n_id)\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        \n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"downloaded_xls\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "        \n",
    "        self.wait_for_id(\"resetbtn\")\n",
    "        \n",
    "        self.click_element_id(\"resetbtn\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            #self.driver.close()\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            #self.driver.close()\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        #self.driver.close()\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def expand_shadow_element(self, element):\n",
    "        shadow_root = self.driver.execute_script('return arguments[0].shadowRoot', element)\n",
    "        return shadow_root\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        \"\"\"\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        \n",
    "        self.driver.get(\"chrome://settings/security\")\n",
    "        \n",
    "\n",
    "        \n",
    "        root = self.driver.find_element_by_tag_name(\"settings-ui\")\n",
    "        shadow_root = self.expand_shadow_element(root)\n",
    "        \n",
    "        root1 = shadow_root.find_element_by_tag_name(\"settings-main\")\n",
    "        shadow_root1 = self.expand_shadow_element(root1)\n",
    "        \n",
    "        root2 = shadow_root1.find_element_by_tag_name(\"settings-basic-page\")\n",
    "        shadow_root2 = self.expand_shadow_element(root2)\n",
    "        \n",
    "        root3 = shadow_root2.find_element_by_tag_name(\"settings-privacy-page\")\n",
    "        shadow_root3 = self.expand_shadow_element(root3)\n",
    "        \n",
    "        root4 = shadow_root3.find_element_by_tag_name(\"settings-security-page\")\n",
    "        shadow_root4 = self.expand_shadow_element(root4)\n",
    "        \n",
    "        root5 = shadow_root4.find_element_by_css_selector(\"#safeBrowsingStandard\")\n",
    "        shadow_root5 = self.expand_shadow_element(root5)\n",
    "        \n",
    "        security_button = shadow_root5.find_element_by_css_selector(\"div.disc-border\")\n",
    "        \n",
    "        security_button.click()\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\",2)\n",
    "        fp.set_preference(\"browser.download.dir\", self.paths['cwd'] + self.paths['sel_xls_download_path'])\n",
    "        \"\"\"\n",
    "        #print(self.paths['cwd'] + self.paths['sel_xls_download_path'])\n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\", 2)\n",
    "        fp.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "        #temp_path = f'⁦C:\\\\Users\\\\Ethan\\\\Documents\\\\Python\\\\Polar Bar Chart\\\\sabioawesome\\\\scraping-test_file\\\\downloaded_xls⁩'\n",
    "        #fp.set_preference(\"browser.download.dir\", self.paths[\"cwd\"] + self.paths[\"sel_xls_download_path\"])\n",
    "        print( self.paths[\"sel_xls_download_path\"])\n",
    "        fp.set_preference(\"browser.download.dir\", self.paths[\"sel_xls_download_path\"])\n",
    "        fp.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/octet-stream\")\n",
    "        self.driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\") \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        \n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "\n",
    "\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "                    \n",
    "                self.count += 1\n",
    "                print(\"Reactions searched: \" + str(self.count) + \"/\" + str(len(self.model[\"reactions\"])), end='\\r')\n",
    "                    \n",
    "                    \n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "#         scraped_sans_parentheses_enzymes = glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        scraped_sans_parentheses_enzymes = glob(os.path.join(self.paths['xls_download_path'], '*.xls'))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\",2)\n",
    "        fp.set_preference(\"browser.download.dir\", self.paths['cwd'] + self.paths['sel_xls_download_path'])\n",
    "        self.driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\") \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in self.variables['scraped_entryids']:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = self.scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(self.paths[\"(scraped_entryids_file_path\"], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(self.paths[\"entryids_json_file_path\"], 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(self.paths[\"scraped_model_json_file_path\"], 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./BiGG_models/BiGG model of S. aureus.json\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self):           \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,source='sabio',): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"xls_download\")\n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "            \n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\",2)\n",
    "        self.driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\") \n",
    "        \n",
    "        if source == 'sabio':\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            self.source = 'sabio'\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        self.count += 1\n",
    "        print(self.count, end='\\r')\n",
    "        \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "        self.click_element_id(\"resetbtn\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            #self.driver.close()\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            #self.driver.close()\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        #self.driver.close()\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def expand_shadow_element(self, element):\n",
    "        shadow_root = self.driver.execute_script('return arguments[0].shadowRoot', element)\n",
    "        return shadow_root\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        \"\"\"\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        \n",
    "        self.driver.get(\"chrome://settings/security\")\n",
    "        \n",
    "\n",
    "        \n",
    "        root = self.driver.find_element_by_tag_name(\"settings-ui\")\n",
    "        shadow_root = self.expand_shadow_element(root)\n",
    "        \n",
    "        root1 = shadow_root.find_element_by_tag_name(\"settings-main\")\n",
    "        shadow_root1 = self.expand_shadow_element(root1)\n",
    "        \n",
    "        root2 = shadow_root1.find_element_by_tag_name(\"settings-basic-page\")\n",
    "        shadow_root2 = self.expand_shadow_element(root2)\n",
    "        \n",
    "        root3 = shadow_root2.find_element_by_tag_name(\"settings-privacy-page\")\n",
    "        shadow_root3 = self.expand_shadow_element(root3)\n",
    "        \n",
    "        root4 = shadow_root3.find_element_by_tag_name(\"settings-security-page\")\n",
    "        shadow_root4 = self.expand_shadow_element(root4)\n",
    "        \n",
    "        root5 = shadow_root4.find_element_by_css_selector(\"#safeBrowsingStandard\")\n",
    "        shadow_root5 = self.expand_shadow_element(root5)\n",
    "        \n",
    "        security_button = shadow_root5.find_element_by_css_selector(\"div.disc-border\")\n",
    "        \n",
    "        security_button.click()\n",
    "        \"\"\"\n",
    "        \n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\",2)\n",
    "        fp.set_preference(\"browser.download.dir\", self.paths['cwd'] + self.paths['sel_xls_download_path'])\n",
    "        driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\") \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        \n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "\n",
    "\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "                    \n",
    "                    \n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "fp.set_preference(\"browser.download.folderList\",2)\n",
    "fp.set_preference(\"browser.download.dir\", os.getcwd())\n",
    "driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chrome driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify the BIGG Model JSON file path: ./BiGG_models/BiGG model of S. aureus.json\n",
      "C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\dFBApy\\scraping\\SABIO\\BiGG_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:200: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5613\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e3274c8e69ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscraping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSABIO_scraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscraping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-aec5b6801a26>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_bigg_xls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob_xls_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_entryids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-aec5b6801a26>\u001b[0m in \u001b[0;36mglob_xls_files\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;31m# combine the total set of dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_dataframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_models\\BiGG model of S. aureus.json\n",
    "# ./BiGG_models/BiGG model of S. aureus.json\n",
    "\n",
    "scraping = SABIO_scraping()\n",
    "scraping.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self,source='sabio'):\n",
    "        driver = webdriver.Firefox(executable_path=r\".\\geckodriver.exe\")\n",
    "        if source == 'sabio':\n",
    "            driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            self.source = 'sabio'\n",
    "            \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"xls_download\")\n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        self.count += 1\n",
    "        print(self.count, end='\\r')\n",
    "        \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "        self.click_element_id(\"resetbtn\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def expand_shadow_element(self, element):\n",
    "        shadow_root = self.driver.execute_script('return arguments[0].shadowRoot', element)\n",
    "        return shadow_root\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        \n",
    "        self.driver.get(\"chrome://settings/security\")\n",
    "\n",
    "        \n",
    "        root = self.driver.find_element_by_tag_name(\"settings-ui\")\n",
    "        shadow_root = self.expand_shadow_element(root)\n",
    "        \n",
    "        root1 = shadow_root.find_element_by_tag_name(\"settings-main\")\n",
    "        shadow_root1 = self.expand_shadow_element(root1)\n",
    "        \n",
    "        root2 = shadow_root1.find_element_by_tag_name(\"settings-basic-page\")\n",
    "        shadow_root2 = self.expand_shadow_element(root2)\n",
    "        \n",
    "        root3 = shadow_root2.find_element_by_tag_name(\"settings-privacy-page\")\n",
    "        shadow_root3 = self.expand_shadow_element(root3)\n",
    "        \n",
    "        root4 = shadow_root3.find_element_by_tag_name(\"settings-security-page\")\n",
    "        shadow_root4 = self.expand_shadow_element(root4)\n",
    "        \n",
    "        root5 = shadow_root4.find_element_by_css_selector(\"#safeBrowsingStandard\")\n",
    "        shadow_root5 = self.expand_shadow_element(root5)\n",
    "        \n",
    "        security_button = shadow_root5.find_element_by_css_selector(\"div.disc-border\")\n",
    "        \n",
    "        security_button.click()\n",
    "\n",
    "\n",
    "        \n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "\n",
    "\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "                    \n",
    "                    \n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self,source='sabio'):\n",
    "        driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        if source == 'sabio':\n",
    "            driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            self.source = 'sabio'\n",
    "            \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"xls_download\")\n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        self.count += 1\n",
    "        print(self.count, end='\\r')\n",
    "        \n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path'], \"download.prompt_for_download\": False, \"download.directory_upgrade\": True, \"safebrowsing.enabled\": True}\n",
    "\n",
    "#         prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)    \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            self.driver.close()\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            self.driver.close()\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "#                     success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self,source='sabio'):\n",
    "        driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        if source == 'sabio':\n",
    "            driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            self.source = 'sabio'\n",
    "            \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"xls_download\")\n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        self.count += 1\n",
    "        print(self.count, end='\\r')\n",
    "        \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "        self.click_element_id(\"resetbtn\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            self.driver.close()\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            #self.driver.close()\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        #self.driver.close()\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        \n",
    "        self.driver.get(\"chrome://settings/security?search=downloads\")\n",
    "        element = self.driver.find_element_by_xpath(\"//div[contains(@class, 'disc-border')]\")\n",
    "        element.click()\n",
    "        \n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "#                     success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify the BIGG Model JSON file path: ./BiGG_models/BiGG model of S. aureus.json\n",
      "C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\dFBApy\\scraping\\SABIO\\BiGG_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:200: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
